<snippet>
  <content><![CDATA[
  s3: >
    aws_secret_key=${1:None #  AWS secret key. If not set then the value of the AWS_SECRET_KEY environment variable is used.}
    src=${2:None #  The source file path when performing a PUT operation.}
    aws_access_key=${3:None #  AWS access key. If not set then the value of the AWS_ACCESS_KEY environment variable is used.}
    dest=${4: #  The destination file path when downloading an object/key with a GET operation.}
    object=${5:None #  Keyname of the object inside the bucket. Can be used to create "virtual directories", see examples.}
    bucket=${6:None # ! Bucket name.}
    expiration=${7:600 #  Time limit (in seconds) for the URL generated and returned by S3/Walrus when performing a mode=put or mode=geturl operation.}
    s3_url=${8:None #  S3 URL endpoint. If not specified then the S3_URL environment variable is used, if that variable is defined. Ansible tries to guess if fakes3 (https://github.com/jubos/fake-s3) or Eucalyptus Walrus (https://github.com/eucalyptus/eucalyptus/wiki/Walrus) is used and configure connection accordingly. Current heuristic is: everything with scheme fakes3:// is fakes3, everything else not ending with amazonaws.com is Walrus.}
    metadata=${9:None #  Metadata for PUT operation, as a dictionary of 'key=value' and 'key=value,key=value'.}
    overwrite=${10:True #  Force overwrite either locally on the filesystem or remotely with the object/key. Used with PUT and GET operations.}
    mode=${11:None # ! Switches the module behaviour between put (upload), get (download), geturl (return download url (Ansible 1.3+), getstr (download object as string (1.3+)), create (bucket) and delete (bucket).}
  ]]></content>
  <tabTrigger>an-s3</tabTrigger>
  <scope>source.yaml</scope>
</snippet>
